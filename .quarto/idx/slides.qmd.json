{"title":"LSTM Recurrent Neural Networks","markdown":{"yaml":{"title":"LSTM Recurrent Neural Networks","author":"Moise Brutus - Kevin Waisfeld - Michael Nhliziyo","format":"revealjs"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\n## What is an LSTM?\n\n-   LSTM networks are a type of recurrent neural network that works well with sequential data, such as time series data and text.\n\n-   LSTMs overcome this challenge by regulating the weights with three \"gates.\"\n\n-   We then plan on putting an LSTM model to the test by seeing how accurately it can predict financial time series data.\n\n## Applications of LSTMs\n\n-   LSTM networks were mainly created to solve the exploding/vanishing gradient problem\n\n# Data\n-  For this analysis, we'll be using historical time-series data for Apple, a corporation listed on the NASDAQ Stock Exchange.\n- We chose Apple due to its high trading volume and relatively low volatility\n- Thus, our data set contains 1,257 trading days, the amount of days in the last five years the stock market was at least partially open.\n## Variables\n\n-   Date : This represents a specific trading day that the market was at least partially open.\n\n-   Open : The price at the time of market opening on the given day.\n\n-   High : The highest price that the stock reached on the given day.\n\n-   Low : The lowest price that the stock reached on the given day.\n\n-   Close : The price at the time of market closing on the given day.\n\n## Additional Variables\n\n## Summary and Visualizations\n\n# Methodology\n\n## Neural Networks\n\n## Gates\n\n-   They are composed of four components; the cell,input gate, the output gate and a forget gate.\n-   The gates are responsible for processing and regulating the information flowing in and out of the cell.\n\n## Activation Functions\n\n# Analysis\n"},"formats":{"revealjs":{"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","output-file":"slides.html"},"language":{},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.2.475","auto-stretch":true,"editor":"visual","title":"LSTM Recurrent Neural Networks","author":"Moise Brutus - Kevin Waisfeld - Michael Nhliziyo"}}}}