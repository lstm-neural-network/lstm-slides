{"title":"LSTM Recurrent Neural Networks","markdown":{"yaml":{"title":"LSTM Recurrent Neural Networks","author":"Moise Brutus - Kevin Waisfeld - Michael Nhliziyo","format":"revealjs"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\n## What is an LSTM?\n\n-   Long Short-Term Memory networks are a type of recurrent neural network that works well with sequential data, such as time series data and text.\n\n-   Their capacity to model and understand long range dependencies makes them critical in executing various tasks.\n\n-   Such sequence prediction problems involve predicting the next value based on the previous input.\n\n-   LSTM networks were mainly created to solve the vanishing gradient problem.\n\n## Application of LSTMs\n\n-   We plan on putting an LSTM model to the test by seeing how accurately it can predict financial time series data.\n\n-   While modeling financial data is an extremely difficult task due to the unpredictability of financial markets, we will explore whether they hold any potential to accurately predict short and medium term movements in stock prices.\n\n# Data\n\n## Background on the data\n\n-   For this analysis, we'll be using historical time-series stock data for Apple listed on the NASDAQ Stock Exchange and General Motors, which is listed on the New York Stock Exchange.\n\n-   The data is sourced from Yahoo Finance using the yfinance package in python.\n\n-   The resulting data file is relatively simple, comprised of only seven columns, with each row representing an individual trading day going back to five years.\n\n-   Our resulting dataset contains 1,238 trading days.\n\n## Variables\n\n-   **Date** : This represents a specific trading day that the market was at least partially open.\n\n-   **Open** : The price at the time of market opening on the given day.\n\n-   **High** : The highest price that the stock reached on the given day.\n\n-   **Low** : The lowest price that the stock reached on the given day.\n\n-   **Close** : The price at the time of market closing on the given day.\n\n-   Adj Close : The closing stock price adjusted for any corporate actions such as dividends and stock splits.\n\n-   Volume : The total number of shares traded on the given day.\n\n## Additional Variables\n\n-   In addition to the variables to the standard variables fetched from the Yahoo Finance API, we created our own columns to add to the dataset.\n\n-   Percent Change : This variable calculates what percent the price changed from the previous closing price.\n\n-   Simple Moving Average : A simple moving average (SMA) is created by taking the mean closing price of a stock over a given number of periods.\n\n-   Exponential Moving Average : EMA's give additional weight to the most recent days.\n\n## Data Summary\n\n![](/datasummary.png){fig-align=\"center\" width=\"733\"}\n\n# Methodology\n\n## RNN\n\n-   Recurrent Neural Networks (RNNs) are a type of artificial neural network designed for processing sequential data.\n\n-   The fundamental idea behind an RNN is to maintain a hidden state vector that gets updated at each time step, incorporating information from both the current input and the previous hidden state.\n\n-   Most ANN's are feed-forward neural networks, so they are all fully connected and loop free.\n\n-   RNN's also need to be trained differently, as backpropgation on its own does not feed any information back to previous steps.\n\n## RNN\n\n-   Unlike ANN's, a RNN uses a circular connection to previous steps, therefore going back in time.\n\n    ![](/RNN.JPG)\n\n## RNN\n\n-   Traditional RNNS still run into the issue of vanishing gradients.\n\n-   Vanishing gradient problem occurs when the gradients become too small due to back propagation.\n\n-   It's particularly problematic in deep networks where gradients can diminish exponentially as they are backpropagated through time, making it difficult for the model to learn long-range dependencies in sequential data.\n\n-   An LSTM is a form of RNN.\n\n## Activation Functions\n\n-   The sigmoid and tanh activation functions are crucial for the different gates and operations of the LSTM architecture in a network.\n\n    ![](/activationfun.png){fig-align=\"center\" width=\"473\"}\n\n-   The sigmoid activaton function outputs a value between 0 and 1.\n\n-   The tanh activation function outputs a value between -1 and 1.\n\n## Gates\n\n-   The gates are responsible for processing and regulating the information flowing in and out of the cell.\n\n-   They are composed of three gates;input gate, the output gate and a forget gate.\n\n    ![](/lstm.jpeg){width=\"500\" fig-align=\"center\"}\n\n## Forget Gate\n\n-   The forget gate determines which information is discarded from the previous hidden state and current input.\n\n-   The closer the value of the information passed through the forget is to 0, the more likely is to be forgotten.\n\n    ![](/forget%20gate.JPG){width=\"1162\" fig-align=\"center\"}\n\n## Input Gate\n\n-   The input gate is in charge of regulating which new information points to include in the network's memory cell state.\n\n-   The input gate regulates the information flow by using the sigmoid function and the tanh function.\n\n    ![](/input%20gate.JPG){width=\"1065\" fig-align=\"center\"}\n\n## Output Gate\n\n-   The output gate decides which memory cell data should be carried over to the following time step and added to the network's output\n\n-   It makes use of a sigmoid activation function.\n\n    ![](/output%20gate.JPG){width=\"1065\" fig-align=\"center\"}\n\n# Analysis\n\n## Implentation\n\n-   Python\n-   Pandas\n-   Numpy\n-   Tensorflow\n-   Keras\n-   Matplotlib\n\n## Pre-Processing\n\n-   Standardization\n\n-   Normalization\n\n-   80/20 Shuffled Split\n\n## LSTM Model\n\nLSTM_modelApple = ke.Sequential() LSTM_modelApple.add(LSTM(128, return_sequences=True, input_shape=(LSTM_Xtrain1.shape\\[1\\], 1))) LSTM_modelApple.add(LSTM(64, return_sequences=False)) LSTM_modelApple.add(Dense(25, activation='linear')) LSTM_modelApple.add(Dense(1))\n\n## Model Summary\n\n![](/ag1.png){fig-align=\"center\"}\n\n## Model Summary\n\n![](/image.webp){fig-align=\"center\" width=\"2462\"}\n\n# Results\n\n## Results\n\n-   Model was able to predict results of Apple's stock price exceptionally well\n\n-   While 270 days looks a little messy on a chart, the predicted stock price was very close to the actual stock price on both the first 50 days of the dataset and the last 50\n\n    ![](/sg1.png){fig-align=\"center\"}\n\n## Results Cont.\n\n-   We also wanted to look at how the model would work at predicting days that were completely unseen in the dataset.\n\n-   Our model was based on stock data running through the end of October 2023. So we picked two additional days in November to see how the model performs.\n\n-   We saw that the predicted price for the two days we chose were also very close to the actual price.\n\n    ![](/sg2.JPG.png){width=\"1009\" fig-align=\"center\"}\n\n## Results Cont.\n\n-   During the time-span of our dataset, Apple's stock had relatively low volatility and was a consistently rising stock. So we also wanted to see how the LSTM model performed on a stock that has been on a different trajectory.\n\n-   We also wanted to see it's performance on another industry and sector of the economy, which is how we landed on General Motors.\n\n## Results Cont.\n\n-   During the same timespan, General Motors stock has had far more volatility and has been mostly stagnant.\n\n    ![](/sg3.JPG){width=\"1507\" fig-align=\"center\"}\n\n## Results Cont.\n\n-   We saw the LSTM model perform just as well on the General Motors stock as it did on the Apple stock. We also saw good results when picking two days in November 2023, which were not contained in the original dataset.\n\n    ![](/sg4.JPG){width=\"1054\" fig-align=\"center\"}\n\n## Results Cont.\n\n-   The number of epochs is an important hyperparameter that went into our analysis. With too few epochs, underfitting is a possibility. With too many epochs, overfitting the data is also a possibility.\n\n-   We were able to run some tests with our data to see how the data performs based on the number of epochs selected.\n\n## Results Cont.\n\n-   We see both the underfitting and overfitting dilemma when looking at both extremes, only running 25 epochs or running 200.\n\n    ![](/sg5.JPG){fig-align=\"center\"}\n\n# Conclusion\n\n## Conclusion\n\n-   We were able to build an LSTM model to accurately predict the price of a stock based on it's previous history, including previous closing prices and trading volume.\n\n-   We tested our hyperparameters to find the best options for our model and adjusted it accordingly.\n\n-   We were also able to replicate the success of our model using another, unrelated stock which has had a significantly different history.\n\n-   Ultimately, our success shows that it might be possible to accurately predict stock market data using a time-series analysis such as LSTM.\n\n## Conclusion Cont.\n\n-   At the same time, there are some caveats to consider. While the prediction price is very close to the actual price, there is a difference. In commodity markets, a small difference can make a huge impact on whether trades are successful or not, especially in short-term trading.\n\n-   Just because we can get close to a prediction price doesn't mean that using such a model is a wise trading strategy (which we would never suggest anyways!)\n\n-   More research is required on using LSTM models in the financial markets, including during times of recession and economic downturn.\n\n## Conclusion Cont.\n\n-   Another caveat to consider is what the LSTM model is using to make its predictions. As it's a time-series analysis, the LSTM model is using history from previous trading days. It's then spitting out a result that very closely mimics the previous days price. In the long-term, predicting that tomorrow's closing price will be very close to today's closing price is generally going to be successful, but it doesn't make it a good trading strategy.\n\n-   When we overlay a line of a stock's closing price with another line of it's one-day closing price (that is, the closing price the previous day), we see quite good results.\n\n## Conclusion Cont.\n\n-   Ultimately, our research and experiment has found that LSTM models do possess predictive capability when it comes to the financial markets, but that more research must be done before such models can have a major impact on financial markets.\n\n    ![](/last%20slide.JPG)\n"},"formats":{"revealjs":{"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","output-file":"slides.html"},"language":{},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.2.475","auto-stretch":true,"editor":"visual","title":"LSTM Recurrent Neural Networks","author":"Moise Brutus - Kevin Waisfeld - Michael Nhliziyo"}}}}